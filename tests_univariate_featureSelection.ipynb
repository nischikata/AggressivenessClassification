{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Utils.setupDataset import get_dataset, combine_datasets\n",
    "from Utils.split_dataset import separateByCategory, get_testSet_validationSet, getTestSets\n",
    "from Utils.feature_ranking import getTopFeatures\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as skmetrics\n",
    "from Utils.selection_metrics import SelectionMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m_dataset = get_dataset(\"Datasets/dataset.pickle\")\n",
    "w_dataset = get_dataset(\"Datasets/wiki_dataset.pickle\")\n",
    "\n",
    "c_dataset = combine_datasets(m_dataset, w_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_selectedFeatures(X, indices = []):\n",
    "    \"\"\"\n",
    "    Returns the feature vector for the given Comment\n",
    "    :param X: the data part of the dataset (array of feature vectors)\n",
    "    :param indices: a list of the (feature column) indices to select\n",
    "    :return: numpy array of selected features\n",
    "    \"\"\"\n",
    "    # select column indices\n",
    "    return X[:, indices]\n",
    "\n",
    "def concat_datasetList(setlist):\n",
    "    \n",
    "    #import pdb; pdb.set_trace()\n",
    "    data = setlist[0][\"data\"]\n",
    "    target = setlist[0][\"target\"]\n",
    "    \n",
    "    for s in setlist[1:]:\n",
    "        data = np.concatenate([data, s[\"data\"]])\n",
    "        target = np.concatenate([target, s[\"target\"]])\n",
    "    \n",
    "    return data, target\n",
    "\n",
    "def get_train_test_set(setlist, index):\n",
    "    X_train, y_train = concat_datasetList(setlist[:index]+setlist[index+1:])\n",
    "    X_test = setlist[index][\"data\"]\n",
    "    y_test = setlist[index][\"target\"]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def get_metrics(X_train, y_train, X_test, y_test):\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)   \n",
    "\n",
    "    confusion = skmetrics.confusion_matrix(y_test, y_pred)\n",
    "    TP = confusion[1,1]\n",
    "    TN = confusion[0,0]\n",
    "    FP = confusion[0,1]\n",
    "    FN = confusion[1,0]\n",
    "    accuracy = skmetrics.accuracy_score(y_test, y_pred)\n",
    "    recall = skmetrics.recall_score(y_test,y_pred)\n",
    "\n",
    "    precision = skmetrics.precision_score(y_test, y_pred)\n",
    "    f1 = 2*precision*recall/(precision+recall)\n",
    "    metrics = np.array([ f1, precision, recall, accuracy, TN, FP, FN, TP])\n",
    "    return metrics\n",
    "\n",
    "def get_metrics2ss(X_train, y_train, X_test, y_test):\n",
    "        model = LogisticRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        return compute_metrics(y_test, y_pred)\n",
    "\n",
    "def metrics_feature_selection(ranks, X_train, y_train, X_test, y_test, n):\n",
    "    # 1. get feature ranks in an array\n",
    "    # loop over array of ranks (COLUMNS!)\n",
    "    metrics_noSelection = get_metrics(X_train, y_train, X_test, y_test)\n",
    "    metrics = [metrics_noSelection]\n",
    "    \n",
    "    \n",
    "    for rank in ranks.T:  # iterate over transposed array (over the columns)\n",
    "        #model = LogisticRegression()\n",
    "        X_train_selection = get_selectedFeatures(X_train, rank[:n]) #apply feature selection according to current ranking\n",
    "        \n",
    "        #model.fit(X_train_selection, y_train)\n",
    "        \n",
    "        X_test_selection = get_selectedFeatures(X_test, rank[:n]) #apply feature selection\n",
    "        #y_pred = model.predict(X_test_selection)\n",
    "    \n",
    "        metrics_temp = get_metrics(X_train_selection, y_train, X_test_selection, y_test)\n",
    "        metrics.append(metrics_temp)\n",
    "  \n",
    "    # returns a list of metrics for each feature selection list for the given train test chunk\n",
    "    return metrics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def featureSelectionResults(dataset, file='rank_selections.cvs', n=25):\n",
    "    # 1. get the feature rankings\n",
    "    ranks = getTopFeatures(dataset, file)\n",
    "    # 2. separate into test validation sets    \n",
    "    good, bad = separateByCategory(dataset)\n",
    "    # good, bad, validate = get_testSet_validationSet(good, bad) brauch kein validation set momentan, oder?\n",
    "    sets = getTestSets(good, bad) #  returns a list of k datasets (each  { \"data\": data, \"target\": target})\n",
    "    r_len = len(ranks.T) + 1 # +1 for result without feature selection\n",
    "    metrics = [np.zeros(8) for i in range(r_len)]\n",
    "    \n",
    "    for index, set in enumerate(sets):\n",
    "        #actually each set is once the testing set, and then the others are used for training\n",
    "        # 1. the index set is the testing set, \n",
    "        X_train, y_train, X_test, y_test = get_train_test_set(sets, index)\n",
    "\n",
    "        \n",
    "        metrics_temp = metrics_feature_selection(ranks, X_train, y_train, X_test, y_test, n)\n",
    "        metrics = [metrics[i]+metrics_temp[i] for i in range(r_len)] #sum up the metrics (compute avg later)\n",
    "\n",
    "    \n",
    "    # TODO: now divide all values of the metrics via the number of sets\n",
    "    set_len = len(sets)\n",
    "    avg_mask = np.array([set_len, set_len, set_len, set_len, 1, 1, 1, 1])\n",
    "    metrics = [m/avg_mask for m in metrics] #divide by the number of sets to get the average metrics\n",
    "    \n",
    "    sm = SelectionMetrics(metrics, ranks, n)\n",
    "    return sm\n",
    "\n",
    "\n",
    "filename = 'rankSelections_combined.csv'\n",
    "sm = featureSelectionResults(c_dataset, filename, 15)\n",
    "df = sm.data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['selection'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('f1', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
